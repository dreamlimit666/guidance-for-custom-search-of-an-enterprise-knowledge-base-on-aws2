{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f2c403",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade boto3 -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "!pip install --upgrade sagemaker -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd48de42-510d-48bb-bd06-c3c0b9e234a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For notebook instances (Amazon Linux)\n",
    "!sudo yum update -y\n",
    "!sudo yum install amazon-linux-extras\n",
    "!sudo amazon-linux-extras install epel -y\n",
    "!sudo yum update -y\n",
    "!sudo yum install git-lfs git -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d741d275",
   "metadata": {},
   "source": [
    "将s3_dir修改为你希望存储模型文件的s3目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1790993-fff6-45b5-a0af-3836bb4cacec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "repository = \"Baichuan-inc/Baichuan2-13B-Chat-4bits\"\n",
    "url_suffix = repository + \".git\"\n",
    "model_id=repository.split(\"/\")[-1]\n",
    "s3_dir = \"s3://your-bucket-name/your-s3-dir\" #specify your s3 dir to store model repo example-> s3://your-bucket-name/your-s3-dir\n",
    "s3_location = s3_dir + \"/\" + model_id + \"/\"\n",
    "repo = \"https://www.wisemodel.cn/\" + url_suffix\n",
    "local_model_dir = \"./\" + model_id + \"/\"\n",
    "\n",
    "!echo $s3_location\n",
    "!echo $repo\n",
    "!echo $local_model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b50830",
   "metadata": {},
   "source": [
    "从 wisemodel.cn下载模型，请确保notebook instance有足够的空间，baichuan2-13b-4bits大概需要16G的空间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83e71b1-dea4-4119-8d87-a1156e21d26f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!git lfs install\n",
    "!git clone $repo\n",
    "#!git clone https://www.wisemodel.cn/Baichuan-inc/Baichuan2-7B-Chat-4bits.git\n",
    "#!git clone https://www.wisemodel.cn/Baichuan-inc/Baichuan2-7B-Chat-4bits.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab93aa43",
   "metadata": {},
   "source": [
    "上传模型文件到s3目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc33551-46e3-4903-a3b9-4859c4cd1eb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 sync $local_model_dir $s3_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63d4b444-6c9e-4226-856d-ca9cefd6c00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938437c5-4d1e-4549-b008-fca292ff0afa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile code/requirements.txt\n",
    "\n",
    "-i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "\n",
    "diffusers\n",
    "ftfy\n",
    "spacy\n",
    "boto3\n",
    "sagemaker\n",
    "nvgpu\n",
    "sentencepiece\n",
    "protobuf>=3.19.5,<3.20.1\n",
    "transformers>=4.26.1\n",
    "icetk\n",
    "cpm_kernels\n",
    "accelerate\n",
    "colorama\n",
    "bitsandbytes\n",
    "transformers_stream_generator\n",
    "xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87518e1b-7c32-4768-909c-0ecce09b1f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/inference.py\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "# this software and associated documentation files (the \"Software\"), to deal in\n",
    "# the Software without restriction, including without limitation the rights to\n",
    "# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "# the Software, and to permit persons to whom the Software is furnished to do so.\n",
    "\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import io\n",
    "import sys\n",
    "\n",
    "import traceback\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import requests\n",
    "import boto3\n",
    "import sagemaker\n",
    "import torch\n",
    "\n",
    "\n",
    "from torch import autocast\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.generation.utils import GenerationConfig\n",
    "\n",
    "LLM_NAME = \"/opt/amazon/var/run/\"\n",
    "s3_location = \"s3://your-bucket-name/your-s3-dir\" #填入前面步骤里的s3_location的值, example-> s3://your-bucket-name/your-s3-dir\n",
    "\n",
    "os.system(f\"aws s3 sync {s3_location} {LLM_NAME}\")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLM_NAME, trust_remote_code=True)\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.replace(\"\\n\", \"\\\\n\").replace(\"\\t\", \"\\\\t\")\n",
    "    return text\n",
    "\n",
    "def postprocess(text):\n",
    "    return text.replace(\"\\\\n\", \"\\n\").replace(\"\\\\t\", \"\\t\")\n",
    "\n",
    "def answer(text, sample=True, top_p=0.45, temperature=0.01, model=None):\n",
    "    text = preprocess(text)\n",
    "    messages = []\n",
    "    messages.append({\"role\": \"user\", \"content\": text})\n",
    "    response = model.chat(tokenizer, messages)\n",
    "        \n",
    "    return postprocess(response)\n",
    "\n",
    "\n",
    "# def model_fn(model_dir):\n",
    "#     \"\"\"\n",
    "#     Load the model for inference,load model from os.environ['model_name'],diffult use stabilityai/stable-diffusion-2\n",
    "    \n",
    "#     \"\"\"\n",
    "#     print(\"=================model_fn_Start=================\")\n",
    "#     model = AutoModelForCausalLM.from_pretrained(LLM_NAME, trust_remote_code=True).half().cuda()\n",
    "#     #model = model.to(\"cuda\")\n",
    "#     print(\"=================model_fn_End=================\")\n",
    "#     return model\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"\n",
    "    Load the model for inference,load model from os.environ['model_name'],diffult use stabilityai/stable-diffusion-2\n",
    "    \n",
    "    \"\"\"\n",
    "    print(\"=================model_fn_Start=================\")\n",
    "    # model = AutoModelForCausalLM.from_pretrained(LLM_NAME, torch_dtype=torch.float16,\n",
    "    #                                              trust_remote_code=True)\n",
    "    # model = model.quantize(4).cuda()\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(LLM_NAME, device_map=\"auto\",\n",
    "                                                 trust_remote_code=True)\n",
    "    # model = AutoModelForCausalLM.from_pretrained(LLM_NAME, device_map=\"auto\",\n",
    "    #                                              torch_dtype=torch.bfloat16, trust_remote_code=True)\n",
    "    model.generation_config = GenerationConfig.from_pretrained(LLM_NAME)\n",
    "    print(\"=================model_fn_End=================\")\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    \"\"\"\n",
    "    Deserialize and prepare the prediction input\n",
    "    \"\"\"\n",
    "    # {\n",
    "    # \"ask\": \"写一个文章，题目是未来城市\"\n",
    "    # }\n",
    "    print(f\"=================input_fn=================\\n{request_content_type}\\n{request_body}\")\n",
    "    input_data = json.loads(request_body)\n",
    "    if 'ask' not in input_data:\n",
    "        input_data['ask']=\"写一个文章，题目是未来城市\"\n",
    "    return input_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    \"\"\"\n",
    "    Apply model to the incoming request\n",
    "    \"\"\"\n",
    "    print(\"=================predict_fn=================\")\n",
    "   \n",
    "    print('input_data: ', input_data)\n",
    "    \n",
    "\n",
    "    try:\n",
    "        #if 'history' not in input_data:\n",
    "        #    history = []\n",
    "        #else:\n",
    "        #    history = input_data['history']\n",
    "        if 'temperature' not in input_data:\n",
    "            temperature = 0.01\n",
    "        else:\n",
    "            temperature = input_data['temperature']\n",
    "        #result, history = answer(input_data['ask'], history=history, model=model)\n",
    "        result = answer(input_data['ask'], model=model)\n",
    "        print(f'====result {result}====')\n",
    "        return result\n",
    "        \n",
    "    except Exception as ex:\n",
    "        traceback.print_exc(file=sys.stdout)\n",
    "        print(f\"=================Exception================={ex}\")\n",
    "\n",
    "    return 'Not found answer'\n",
    "\n",
    "\n",
    "def output_fn(prediction, content_type):\n",
    "    \"\"\"\n",
    "    Serialize and prepare the prediction output\n",
    "    \"\"\"\n",
    "    print(content_type)\n",
    "    return json.dumps(\n",
    "        {\n",
    "            'answer': prediction\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76deb1bf-fece-45b2-8c0c-ab204fb5a3b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region_name = boto3.session.Session().region_name\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "print(role)\n",
    "print(bucket)\n",
    "print(region_name)\n",
    "\n",
    "\n",
    "if \"cn-\" in region_name:\n",
    "    with open('./code/requirements.txt', 'r') as original: data = original.read()\n",
    "    with open('./code/requirements.txt', 'w') as modified: modified.write(\"-i https://pypi.tuna.tsinghua.edu.cn/simple\\n\" + data)\n",
    "\n",
    "!touch dummy\n",
    "!tar czvf model.tar.gz dummy\n",
    "assets_dir = 's3://{0}/{1}/assets/'.format(bucket, 'llm_chinese')\n",
    "model_data = 's3://{0}/{1}/assets/model.tar.gz'.format(bucket, 'llm_chinese')\n",
    "!aws s3 cp model.tar.gz $assets_dir\n",
    "!rm -f dummy model.tar.gz\n",
    "\n",
    "model_name = None\n",
    "entry_point = 'inference.py'\n",
    "# framework_version = '1.13.1'\n",
    "# py_version = 'py39'\n",
    "model_environment = {\n",
    "    'SAGEMAKER_MODEL_SERVER_TIMEOUT':'420', \n",
    "    'SAGEMAKER_MODEL_SERVER_WORKERS': '1', \n",
    "}\n",
    "\n",
    "url = f'763104351884.dkr.ecr.{region_name}.amazonaws.com/huggingface-pytorch-inference:2.0.0-transformers4.28.1-gpu-py310-cu118-ubuntu20.04'\n",
    "if \"cn-\" in region_name:\n",
    "    url = f'727897471807.dkr.ecr.{region_name}.amazonaws.com.cn/huggingface-pytorch-inference:2.0.0-transformers4.28.1-gpu-py310-cu118-ubuntu20.04'\n",
    "    \n",
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "model = HuggingFaceModel(\n",
    "    name = model_name,\n",
    "    model_data = model_data,\n",
    "    entry_point = entry_point,\n",
    "    source_dir = './code',\n",
    "    role = role,\n",
    "    # framework_version = framework_version, \n",
    "    # py_version = py_version,\n",
    "    # env = model_environment\n",
    "    image_uri=url\n",
    ")\n",
    "\n",
    "#endpoint_name = 'hf-inference-baichuan-v1'\n",
    "endpoint_name = 'pytorch-inference-llm-v1'\n",
    "# instance_type='ml.g5.4xlarge' \n",
    "instance_type='ml.g4dn.4xlarge' \n",
    "\n",
    "instance_count = 1\n",
    "\n",
    "\n",
    "import boto3\n",
    "\n",
    "client = boto3.client('sagemaker')\n",
    "try:\n",
    "    response = client.delete_endpoint_config(EndpointConfigName=endpoint_name)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "predictor = model.deploy(\n",
    "    endpoint_name = endpoint_name,\n",
    "    instance_type = instance_type, \n",
    "    initial_instance_count = instance_count,\n",
    "    serializer = JSONSerializer(),\n",
    "    deserializer = JSONDeserializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f59e3f",
   "metadata": {},
   "source": [
    "### 测试\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ada7d8b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#休眠2分钟,确保模型可以完全加载\n",
    "import time\n",
    "time.sleep(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e48e6e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好！有什么我可以帮助你的？\n",
      "1.保持安静的环境：尽量让房间保持安静，避免噪音干扰。\n",
      "2. 调整温度和湿度：保持室内温度和湿度适中，有助于入睡。\n",
      "3. 调整光线：避免过亮的光线影响睡眠，可以使用遮光窗帘或眼罩来遮挡光线。\n",
      "4. 放松身心：可以尝试一些放松身心的方法，如深呼吸、冥想等。\n",
      "5. 适当运动：适当的运动可以帮助身体释放压力，促进睡眠。但要注意不要选择过于激烈的运动，以免让身体过于兴奋。\n",
      "6. 避免过度刺激大脑：尽量避免看刺激性强的电视节目或者玩刺激性的游戏，以免大脑过于兴奋难以入睡。\n",
      "7. 限制咖啡因和酒精的摄入：尽量避免在晚上摄入过多的咖啡因和酒精，这些物质会影响睡眠质量。\n",
      "8. 建立规律的作息：尽量保持每天同一时间上床睡觉和起床，有助于调整生物钟，提高睡眠质量。\n"
     ]
    }
   ],
   "source": [
    "inputs= {\n",
    "    \"ask\": \"你好!\"\n",
    "\n",
    "}\n",
    "\n",
    "response = predictor.predict(inputs)\n",
    "print(response[\"answer\"])\n",
    "\n",
    "inputs= {\n",
    "    \"ask\": \"晚上睡不着应该怎么办\"\n",
    "\n",
    "}\n",
    "\n",
    "response = predictor.predict(inputs)\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2dcc6a",
   "metadata": {},
   "source": [
    "### 删除SageMaker  Endpoint\n",
    "删除推理服务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c329e2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
